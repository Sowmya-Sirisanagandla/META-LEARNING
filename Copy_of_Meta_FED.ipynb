{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jahnavi927/Metabridge/blob/main/Copy_of_Meta_FED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSbuTCvxsXj1",
        "outputId": "1cfffaae-e0bb-4fde-9449-97427f36c192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes_val.csv → ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "heart_trainA.csv → ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target', 'Subject']\n",
            "diabetes_trainA.csv → ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "heart_trainB.csv → ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target', 'Subject']\n",
            "processed.cleveland.csv → ['63.0', '1.0', '1.0.1', '145.0', '233.0', '1.0.2', '2.0', '150.0', '0.0', '2.3', '3.0', '0.0.1', '6.0', '0']\n",
            "heart_test.csv → ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target', 'Subject']\n",
            "diabetes_trainB.csv → ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "AZT1D_CGM_Labeled.csv → ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "diabetes_test.csv → ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "heart_val.csv → ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target', 'Subject']\n"
          ]
        }
      ],
      "source": [
        "#MERGING FILES\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "csv_files = glob.glob(\"*.csv\")\n",
        "\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file, nrows=5)  # just read 5 rows to check columns\n",
        "        print(f\"{file} → {df.columns.tolist()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sFMnFBAIcGU",
        "outputId": "ddf1f669-9627-433b-93d1-0e891ad8e8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessed dataset saved with headers and missing values handled in 'cleveland_preprocessed.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load raw Cleveland data\n",
        "df = pd.read_csv(\"/content/processed.cleveland.csv\", header=None)\n",
        "\n",
        "# Assign headers\n",
        "headers = [\n",
        "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\",\n",
        "    \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\",\n",
        "    \"ca\", \"thal\", \"target\"\n",
        "]\n",
        "df.columns = headers\n",
        "\n",
        "# Replace '?' with NaN\n",
        "df = df.replace('?', np.nan)\n",
        "\n",
        "# Convert all columns to numeric\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col])\n",
        "\n",
        "# Handle missing values (choose one method)\n",
        "# Option 1: Drop rows with missing values\n",
        "df = df.dropna()\n",
        "# Option 2: Fill missing values with median (alternative)\n",
        "# df = df.fillna(df.median())\n",
        "\n",
        "# Save cleaned CSV\n",
        "df.to_csv(\"cleveland_preprocessed.csv\", index=False)\n",
        "print(\"✅ Preprocessed dataset saved with headers and missing values handled in 'cleveland_preprocessed.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhmJG6zh_okV",
        "outputId": "3a32cd23-49ec-4482-bb6d-2539b4f882cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned dataset saved as AZT1D_CGM_Time_CGM_Subject.csv\n",
            "Final shape: (591532, 3)\n",
            "Columns: ['EventDateTime', 'CGM', 'Subject']\n",
            "Unique subjects: 25\n",
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            " 25]\n"
          ]
        }
      ],
      "source": [
        "#FINAL PRE-PROCESSING OF DIABETIS TIME-SERIES DATA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load merged dataset\n",
        "df = pd.read_csv(\"AZT1D_CGM_Labeled.csv\")\n",
        "\n",
        "# Keep only the important columns\n",
        "df = df[[\"EventDateTime\", \"CGM\", \"Subject\"]]\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna(subset=[\"EventDateTime\", \"CGM\"])\n",
        "\n",
        "# Convert CGM to numeric (coerce errors to NaN)\n",
        "df[\"CGM\"] = pd.to_numeric(df[\"CGM\"], errors='coerce')\n",
        "\n",
        "# Fill or drop remaining NaNs in CGM\n",
        "df = df.dropna(subset=[\"CGM\"])  # or use df[\"CGM\"].fillna(df[\"CGM\"].median(), inplace=True)\n",
        "\n",
        "# Optionally, sort by Subject and time\n",
        "df = df.sort_values(by=[\"Subject\", \"EventDateTime\"]).reset_index(drop=True)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"AZT1D_CGM_Time_CGM_Subject.csv\", index=False)\n",
        "\n",
        "print(\"✅ Cleaned dataset saved as AZT1D_CGM_Time_CGM_Subject.csv\")\n",
        "print(\"Final shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Unique subjects:\", df['Subject'].nunique())\n",
        "print(df[\"Subject\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRE2TntPCPc3",
        "outputId": "0329d83a-2b6f-42f1-f93d-b61ac607c590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate rows remain\n",
            "Cleaned & labeled dataset saved as AZT1D_CGM_Labeled.csv\n",
            "Final shape: (591532, 5)\n",
            "Columns: ['EventDateTime', 'CGM', 'Subject', 'Label', 'Label_num']\n",
            "Unique subjects: 25\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"AZT1D_CGM_Labeled.csv\")\n",
        "\n",
        "# Keep only the important columns\n",
        "df = df[[\"EventDateTime\", \"CGM\", \"Subject\"]]\n",
        "\n",
        "# Convert EventDateTime to datetime objects\n",
        "df[\"EventDateTime\"] = pd.to_datetime(df[\"EventDateTime\"], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with invalid datetime or missing CGM\n",
        "df = df.dropna(subset=[\"EventDateTime\", \"CGM\"])\n",
        "\n",
        "# Sort data by Subject and EventDateTime\n",
        "df = df.sort_values(by=[\"Subject\", \"EventDateTime\"]).reset_index(drop=True)\n",
        "# Drop duplicate rows\n",
        "df = df.drop_duplicates(subset=[\"EventDateTime\", \"CGM\", \"Subject\"])\n",
        "\n",
        "# Add Hypo / Normal / Hyper labels\n",
        "def label_glucose(value):\n",
        "    if value < 70:\n",
        "        return \"Hypo\"\n",
        "    elif value > 180:\n",
        "        return \"Hyper\"\n",
        "    else:\n",
        "        return \"Normal\"\n",
        "\n",
        "df[\"Label\"] = df[\"CGM\"].apply(label_glucose)\n",
        "#convert  labels to binary numbers\n",
        "label_map = {\"Hypo\": 1, \"Normal\": 0, \"Hyper\": 2}\n",
        "df[\"Label_num\"] = df[\"Label\"].map(label_map)\n",
        "# Check if any duplicates are still present\n",
        "duplicates = df.duplicated(subset=[\"EventDateTime\", \"CGM\", \"Subject\"]).sum()\n",
        "# check if exists any duplicate rows\n",
        "if duplicates == 0:\n",
        "    print(\"No duplicate rows remain\")\n",
        "else:\n",
        "    print(f\"Found {duplicates} duplicate rows\")\n",
        "\n",
        "# Save the cleaned + labeled dataset\n",
        "df.to_csv(\"AZT1D_CGM_Labeled.csv\", index=False)\n",
        "\n",
        "print(\"Cleaned & labeled dataset saved as AZT1D_CGM_Labeled.csv\")\n",
        "print(\"Final shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Unique subjects:\", df['Subject'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHyPUCc989o",
        "outputId": "a278be6b-22ad-47e6-ea98-8988abdf91ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Shifts created successfully!\n",
            "\n",
            "Cleveland target distributions:\n",
            "\n",
            "Before:\n",
            " target\n",
            "0    0.538721\n",
            "1    0.181818\n",
            "2    0.117845\n",
            "3    0.117845\n",
            "4    0.043771\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "After (Mild):\n",
            " target\n",
            "0    0.508418\n",
            "1    0.198653\n",
            "3    0.117845\n",
            "2    0.111111\n",
            "4    0.063973\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "After (Medium):\n",
            " target\n",
            "0    0.420875\n",
            "1    0.185185\n",
            "2    0.161616\n",
            "3    0.131313\n",
            "4    0.101010\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "After (Strong):\n",
            " target\n",
            "0    0.397306\n",
            "1    0.188552\n",
            "2    0.148148\n",
            "3    0.138047\n",
            "4    0.127946\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "AZT1D CGM statistics:\n",
            "\n",
            "Before:\n",
            " count    591532.00\n",
            "mean        146.54\n",
            "std          47.74\n",
            "min          40.00\n",
            "25%         114.00\n",
            "50%         138.00\n",
            "75%         171.00\n",
            "max         400.00\n",
            "Name: CGM, dtype: float64\n",
            "\n",
            "After (Mild):\n",
            " count    591532.00\n",
            "mean        146.54\n",
            "std          47.74\n",
            "min          39.14\n",
            "25%         113.56\n",
            "50%         137.73\n",
            "75%         171.15\n",
            "max         400.62\n",
            "Name: CGM, dtype: float64\n",
            "\n",
            "After (Medium):\n",
            " count    591532.00\n",
            "mean        146.54\n",
            "std          47.75\n",
            "min          33.59\n",
            "25%         113.55\n",
            "50%         137.74\n",
            "75%         171.20\n",
            "max         406.38\n",
            "Name: CGM, dtype: float64\n",
            "\n",
            "After (Strong):\n",
            " count    591532.00\n",
            "mean        146.54\n",
            "std          47.82\n",
            "min          30.07\n",
            "25%         113.37\n",
            "50%         137.83\n",
            "75%         171.42\n",
            "max         408.39\n",
            "Name: CGM, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#APPLYING MEDIUM SHIFTS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Load datasets\n",
        "# -----------------------------\n",
        "cleveland_path = \"cleveland_preprocessed.csv\"\n",
        "aztid_path = \"AZT1D_CGM_Labeled.csv\"\n",
        "\n",
        "if not os.path.exists(cleveland_path):\n",
        "    raise FileNotFoundError(\"Cleveland dataset not found. Please preprocess and save as cleveland_preprocessed.csv\")\n",
        "if not os.path.exists(aztid_path):\n",
        "    raise FileNotFoundError(\"AZT1D dataset not found. Please preprocess and save as AZT1D_CGM_Labeled.csv\")\n",
        "\n",
        "cleveland_df = pd.read_csv(cleveland_path)\n",
        "aztid_df = pd.read_csv(aztid_path)\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def simulate_label_shift_multiclass(df, target_col=\"target\", intensity=\"medium\"):\n",
        "    shifted = df.copy()\n",
        "    n = len(shifted)\n",
        "\n",
        "    frac_map = {\"mild\": 0.1, \"medium\": 0.3, \"strong\": 0.5}\n",
        "    frac = frac_map[intensity]\n",
        "\n",
        "    n_shift = int(n * frac)\n",
        "    idx = np.random.choice(shifted.index, size=n_shift, replace=False)\n",
        "\n",
        "    classes = shifted[target_col].unique()\n",
        "    new_labels = np.random.choice(classes, size=n_shift)\n",
        "    shifted.loc[idx, target_col] = new_labels\n",
        "\n",
        "    return shifted\n",
        "\n",
        "def simulate_feature_shift(df, feature_cols, intensity=\"medium\"):\n",
        "    shifted = df.copy()\n",
        "\n",
        "    std_map = {\"mild\": 0.5, \"medium\": 2.0, \"strong\": 5.0}\n",
        "    noise_std = std_map[intensity]\n",
        "\n",
        "    for col in feature_cols:\n",
        "        if pd.api.types.is_numeric_dtype(shifted[col]):\n",
        "            n_shift = int(len(shifted) * 0.3)  # shift 30% of rows\n",
        "            idx = np.random.choice(shifted.index, size=n_shift, replace=False)\n",
        "            noise = np.random.normal(0, noise_std, n_shift)\n",
        "            shifted.loc[idx, col] = shifted.loc[idx, col] + noise\n",
        "\n",
        "    return shifted\n",
        "\n",
        "\n",
        "# Apply shifts for Cleveland\n",
        "\n",
        "cleveland_versions = {}\n",
        "for intensity in [\"mild\", \"medium\", \"strong\"]:\n",
        "    df_label_shift = simulate_label_shift_multiclass(cleveland_df, target_col=\"target\", intensity=intensity)\n",
        "    df_feature_shift = simulate_feature_shift(\n",
        "        df_label_shift,\n",
        "        feature_cols=[c for c in cleveland_df.columns if c != \"target\"],\n",
        "        intensity=intensity\n",
        "    )\n",
        "    cleveland_versions[intensity] = df_feature_shift\n",
        "    df_feature_shift.to_csv(f\"cleveland_shifted_{intensity}.csv\", index=False)\n",
        "\n",
        "\n",
        "# Apply shifts for AZT1D\n",
        "\n",
        "aztid_versions = {}\n",
        "for intensity in [\"mild\", \"medium\", \"strong\"]:\n",
        "    df_shift = simulate_feature_shift(aztid_df, feature_cols=[\"CGM\"], intensity=intensity)\n",
        "    aztid_versions[intensity] = df_shift\n",
        "    df_shift.to_csv(f\"aztid_shifted_{intensity}.csv\", index=False)\n",
        "\n",
        "# Print BEFORE vs AFTER summaries\n",
        "\n",
        "print(\"\\n Shifts created successfully!\\n\")\n",
        "\n",
        "# Cleveland target distribution\n",
        "print(\"Cleveland target distributions:\")\n",
        "before_cleveland = cleveland_df[\"target\"].value_counts(normalize=True)\n",
        "print(\"\\nBefore:\\n\", before_cleveland)\n",
        "\n",
        "for intensity, df in cleveland_versions.items():\n",
        "    after = df[\"target\"].value_counts(normalize=True)\n",
        "    print(f\"\\nAfter ({intensity.capitalize()}):\\n\", after)\n",
        "\n",
        "# AZT1D CGM stats\n",
        "print(\"\\nAZT1D CGM statistics:\")\n",
        "before_aztid = aztid_df[\"CGM\"].describe().round(2)\n",
        "print(\"\\nBefore:\\n\", before_aztid)\n",
        "\n",
        "for intensity, df in aztid_versions.items():\n",
        "    after = df[\"CGM\"].describe().round(2)\n",
        "    print(f\"\\nAfter ({intensity.capitalize()}):\\n\", after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTPSZwArDLFR",
        "outputId": "5fd48688-19a2-4bb5-9a6d-9fe66ca2db40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Crossed federated datasets created!\n",
            "Federation A: Heart 145 rows (~70%), Diabetes 124218 rows (~30%)\n",
            "Federation B: Heart 62 rows (~30%), Diabetes 289843 rows (~70%)\n",
            "Global Validation: Heart 45, Diabetes 88719\n",
            "Global Testing: Heart 45, Diabetes 88752\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Load datasets\n",
        "# ------------------------------\n",
        "heart_data = pd.read_csv(\"cleveland_shifted_medium.csv\")\n",
        "diab_data = pd.read_csv(\"aztid_shifted_medium.csv\")\n",
        "\n",
        "# If heart dataset has no 'Subject', create one\n",
        "if \"Subject\" not in heart_data.columns:\n",
        "    heart_data[\"Subject\"] = heart_data.index\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: Split global train/val/test\n",
        "# ------------------------------\n",
        "# Heart\n",
        "heart_train, heart_temp = train_test_split(\n",
        "    heart_data, test_size=0.3, random_state=42, stratify=heart_data[\"target\"]\n",
        ")\n",
        "heart_val, heart_test = train_test_split(\n",
        "    heart_temp, test_size=0.5, random_state=42, stratify=heart_temp[\"target\"]\n",
        ")\n",
        "\n",
        "# Diabetes (participant-wise)\n",
        "def split_timeseries(df, participant_col=\"Subject\", frac_train=0.7, frac_val=0.15):\n",
        "    train_list, val_list, test_list = [], [], []\n",
        "    for pid, group in df.groupby(participant_col):\n",
        "        n = len(group)\n",
        "        n_train = int(n * frac_train)\n",
        "        n_val = int(n * frac_val)\n",
        "        train_list.append(group.iloc[:n_train])\n",
        "        val_list.append(group.iloc[n_train:n_train+n_val])\n",
        "        test_list.append(group.iloc[n_train+n_val:])\n",
        "    return pd.concat(train_list), pd.concat(val_list), pd.concat(test_list)\n",
        "\n",
        "diab_train, diab_val, diab_test = split_timeseries(diab_data)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Create crossed federations\n",
        "# ------------------------------\n",
        "# Federation A: 70% heart + 30% diabetes\n",
        "fedA_heart_train = heart_train.sample(frac=0.7, random_state=42)\n",
        "fedB_heart_train = heart_train.drop(fedA_heart_train.index)\n",
        "\n",
        "fedA_diab_train = diab_train.sample(frac=0.3, random_state=42)\n",
        "fedB_diab_train = diab_train.drop(fedA_diab_train.index)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: Create folders\n",
        "# ------------------------------\n",
        "os.makedirs(\"data_federations/federationA\", exist_ok=True)\n",
        "os.makedirs(\"data_federations/federationB\", exist_ok=True)\n",
        "os.makedirs(\"data_federations/validation\", exist_ok=True)\n",
        "os.makedirs(\"data_federations/testing\", exist_ok=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: Save CSVs\n",
        "# ------------------------------\n",
        "fedA_heart_train.to_csv(\"data_federations/federationA/heart_train.csv\", index=False)\n",
        "fedA_diab_train.to_csv(\"data_federations/federationA/diabetes_train.csv\", index=False)\n",
        "\n",
        "fedB_heart_train.to_csv(\"data_federations/federationB/heart_train.csv\", index=False)\n",
        "fedB_diab_train.to_csv(\"data_federations/federationB/diabetes_train.csv\", index=False)\n",
        "\n",
        "# Global validation\n",
        "heart_val.to_csv(\"data_federations/validation/heart_val.csv\", index=False)\n",
        "diab_val.to_csv(\"data_federations/validation/diabetes_val.csv\", index=False)\n",
        "\n",
        "# Global testing\n",
        "heart_test.to_csv(\"data_federations/testing/heart_test.csv\", index=False)\n",
        "diab_test.to_csv(\"data_federations/testing/diabetes_test.csv\", index=False)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 6: Summary\n",
        "# ------------------------------\n",
        "print(\"✅ Crossed federated datasets created!\")\n",
        "print(f\"Federation A: Heart {len(fedA_heart_train)} rows (~70%), Diabetes {len(fedA_diab_train)} rows (~30%)\")\n",
        "print(f\"Federation B: Heart {len(fedB_heart_train)} rows (~30%), Diabetes {len(fedB_diab_train)} rows (~70%)\")\n",
        "print(f\"Global Validation: Heart {len(heart_val)}, Diabetes {len(diab_val)}\")\n",
        "print(f\"Global Testing: Heart {len(heart_test)}, Diabetes {len(diab_test)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_59hBDYD1B6L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzbQiopFwqq1",
        "outputId": "b4f3de30-add3-4a81-e0bb-4f35f7a1bb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LOCAL TRAINING @ A ===\n",
            "Client A ep 1/8 | heart_loss 0.6534 | diab_loss 0.0080\n",
            "Client A ep 2/8 | heart_loss 0.3106 | diab_loss 0.0028\n",
            "Client A ep 3/8 | heart_loss 0.2798 | diab_loss 0.0029\n",
            "Client A ep 4/8 | heart_loss 0.2699 | diab_loss 0.0030\n",
            "Client A ep 5/8 | heart_loss 0.2649 | diab_loss 0.0030\n",
            "Client A ep 6/8 | heart_loss 0.2584 | diab_loss 0.0028\n",
            "Client A ep 7/8 | heart_loss 0.2541 | diab_loss 0.0030\n",
            "Client A ep 8/8 | heart_loss 0.2544 | diab_loss 0.0028\n",
            "\n",
            "=== LOCAL TRAINING @ B ===\n",
            "Client B ep 1/8 | heart_loss 0.3091 | diab_loss 0.0031\n",
            "Client B ep 2/8 | heart_loss 0.2342 | diab_loss 0.0013\n",
            "Client B ep 3/8 | heart_loss 0.2317 | diab_loss 0.0013\n",
            "Client B ep 4/8 | heart_loss 0.2316 | diab_loss 0.0013\n",
            "Client B ep 5/8 | heart_loss 0.2318 | diab_loss 0.0013\n",
            "Client B ep 6/8 | heart_loss 0.2306 | diab_loss 0.0013\n",
            "Client B ep 7/8 | heart_loss 0.2298 | diab_loss 0.0013\n",
            "Client B ep 8/8 | heart_loss 0.2308 | diab_loss 0.0014\n",
            "\n",
            "========== CKD cycle 1/3 ==========\n",
            "\n",
            "--- CKD Teacher→Student (B) ---\n",
            " CKD ep 1/1 | loss 0.1361\n",
            "\n",
            "--- CKD Teacher→Student (A) ---\n",
            " CKD ep 1/1 | loss 0.2029\n",
            "\n",
            "========== CKD cycle 2/3 ==========\n",
            "\n",
            "--- CKD Teacher→Student (B) ---\n",
            " CKD ep 1/1 | loss 0.1304\n",
            "\n",
            "--- CKD Teacher→Student (A) ---\n",
            " CKD ep 1/1 | loss 0.1885\n",
            "\n",
            "========== CKD cycle 3/3 ==========\n",
            "\n",
            "--- CKD Teacher→Student (B) ---\n",
            " CKD ep 1/1 | loss 0.1269\n",
            "\n",
            "--- CKD Teacher→Student (A) ---\n",
            " CKD ep 1/1 | loss 0.1814\n",
            "\n",
            "=== PERSONALIZING A ===\n",
            " Personalize ep 1/6 | heart_loss 0.2328 | diab_loss 0.0028 | val_f1 1.0000\n",
            " Personalize ep 2/6 | heart_loss 0.2301 | diab_loss 0.0023 | val_f1 1.0000\n",
            " Personalize ep 3/6 | heart_loss 0.2304 | diab_loss 0.0022 | val_f1 1.0000\n",
            " Personalize ep 4/6 | heart_loss 0.2305 | diab_loss 0.0021 | val_f1 1.0000\n",
            " Early stopping triggered.\n",
            "Saved: metafed_ckd_artifacts_final/personalized_A_state.pt\n",
            "\n",
            "=== PERSONALIZING B ===\n",
            " Personalize ep 1/6 | heart_loss 0.2275 | diab_loss 0.0011 | val_f1 1.0000\n",
            " Personalize ep 2/6 | heart_loss 0.2275 | diab_loss 0.0010 | val_f1 1.0000\n",
            " Personalize ep 3/6 | heart_loss 0.2268 | diab_loss 0.0009 | val_f1 1.0000\n",
            " Personalize ep 4/6 | heart_loss 0.2271 | diab_loss 0.0009 | val_f1 1.0000\n",
            " Early stopping triggered.\n",
            "Saved: metafed_ckd_artifacts_final/personalized_B_state.pt\n",
            "\n",
            "All stages complete. Artifacts → metafed_ckd_artifacts_final\n",
            "Elapsed (s): 820.1469848155975\n"
          ]
        }
      ],
      "source": [
        "# ========================= Metafed CKD — Training =========================\n",
        "# Model: GRU (seq) + Transformer (seq) + MLP (tabular) → two heads (Heart-5, Diabetes-3)\n",
        "# Stages: Local → CKD cycles → Personalization\n",
        "# Artifacts saved to: ./metafed_ckd_artifacts_final/\n",
        "# =================================================================================\n",
        "import os, time, copy, math, random\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ARTIFACT_DIR = \"metafed_ckd_artifacts_final\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "# input files\n",
        "A_HEART_CSV = \"data_federations/federationA/heart_train.csv\"\n",
        "A_DIAB_CSV  = \"data_federations/federationA/diabetes_train.csv\"\n",
        "B_HEART_CSV = \"data_federations/federationB/heart_train.csv\"\n",
        "B_DIAB_CSV  = \"data_federations/federationB/diabetes_train.csv\"\n",
        "\n",
        "# training hyperparams\n",
        "LOCAL_EPOCHS = 8\n",
        "CKD_CYCLES = 3\n",
        "CKD_EPOCHS_PER_CYCLE = 1\n",
        "PERSONAL_EPOCHS = 6\n",
        "PATIENCE = 3\n",
        "VAL_SPLIT = 0.15\n",
        "\n",
        "BATCH_HEART = 64\n",
        "BATCH_DIAB  = 256\n",
        "\n",
        "LR_BASE = 1e-3\n",
        "LR_CKD  = 5e-4\n",
        "LR_PERS = 3e-4\n",
        "WEIGHT_DECAY = 1e-6\n",
        "CLIP_GRAD = 1.0\n",
        "\n",
        "# KD\n",
        "KD_TEMPERATURE = 2.5\n",
        "KD_WEIGHT = 0.7\n",
        "\n",
        "# diabetes trend labeling\n",
        "DIAB_SEQ_IN = 30\n",
        "DIAB_SLOPE_WINDOW = 12\n",
        "SLOPE_UP = 0.5\n",
        "SLOPE_DOWN = -0.5\n",
        "\n",
        "# model sizes\n",
        "GRU_HIDDEN = 128\n",
        "GRU_LAYERS = 1\n",
        "TRANS_DMODEL = 128\n",
        "TRANS_NHEAD = 4\n",
        "TRANS_LAYERS = 2\n",
        "DROPOUT = 0.25\n",
        "HEART_HIDDEN = [128, 64]\n",
        "DIAB_HEAD_HIDDEN = 64\n",
        "LABEL_SMOOTH = 0.05\n",
        "\n",
        "# seeds\n",
        "SEED = 42\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "\n",
        "# Utilities\n",
        "\n",
        "def compute_trend_labels(df: pd.DataFrame, subj_col=\"Subject\", value_col=\"CGM\"):\n",
        "    \"\"\"Adds CGM_scaled and trend (0=Flat,1=Up,2=Down) using slope on per-subject scaled series.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[value_col] = pd.to_numeric(df[value_col], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[value_col]).reset_index(drop=True)\n",
        "\n",
        "    sub_stats = {}\n",
        "    df[\"CGM_scaled\"] = np.nan\n",
        "    df[\"trend\"] = -1\n",
        "    for subj, g in df.groupby(subj_col):\n",
        "        g = g.sort_values(\"EventDateTime\").reset_index()\n",
        "        vals = g[value_col].values.astype(np.float32)\n",
        "        mu = vals.mean()\n",
        "        sd = vals.std() if vals.std() > 1e-6 else 1.0\n",
        "        sub_stats[subj] = (float(mu), float(sd))\n",
        "        scaled = (vals - mu) / sd\n",
        "        df.loc[g[\"index\"], \"CGM_scaled\"] = scaled\n",
        "\n",
        "        n = DIAB_SLOPE_WINDOW\n",
        "        for i in range(len(scaled)):\n",
        "            if i < n:\n",
        "                df.loc[g[\"index\"][i], \"trend\"] = 0\n",
        "            else:\n",
        "                slope = (scaled[i] - scaled[i-n]) / float(n)\n",
        "                if slope > SLOPE_UP:      lab = 1\n",
        "                elif slope < SLOPE_DOWN:  lab = 2\n",
        "                else:                     lab = 0\n",
        "                df.loc[g[\"index\"][i], \"trend\"] = lab\n",
        "\n",
        "    df = df.dropna(subset=[\"CGM_scaled\"]).reset_index(drop=True)\n",
        "    df[\"trend\"] = df[\"trend\"].astype(int)\n",
        "    return df, sub_stats\n",
        "\n",
        "def make_class_weights(y_int: np.ndarray, num_classes: int = 5, power: float = 0.5) -> torch.Tensor:\n",
        "    \"\"\"Inverse-frequency class weights with smoothing power to avoid extremes.\"\"\"\n",
        "    counts = np.bincount(y_int, minlength=num_classes).astype(np.float32)\n",
        "    counts[counts == 0] = 1.0\n",
        "    inv = (1.0 / counts) ** power\n",
        "    w = inv / inv.mean()\n",
        "    return torch.tensor(w, dtype=torch.float32)\n",
        "\n",
        "# ----------------------------\n",
        "# Datasets\n",
        "# ----------------------------\n",
        "class HeartDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, scaler: StandardScaler = None, fit_scaler=False):\n",
        "        self.df = df.reset_index(drop=True).copy()\n",
        "        X = self.df.drop(columns=[\"target\"], errors=\"ignore\")\n",
        "        if scaler is None and fit_scaler:\n",
        "            self.scaler = StandardScaler().fit(X)\n",
        "            Xs = self.scaler.transform(X)\n",
        "        elif scaler is not None:\n",
        "            self.scaler = scaler\n",
        "            Xs = self.scaler.transform(X)\n",
        "        else:\n",
        "            self.scaler = None\n",
        "            Xs = X.values\n",
        "        self.X = torch.tensor(Xs, dtype=torch.float32)\n",
        "        self.y = torch.tensor(self.df[\"target\"].astype(int).values, dtype=torch.long)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "class DiabetesSeqDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, seq_in=DIAB_SEQ_IN):\n",
        "        self.samples = []\n",
        "        df = df.copy()\n",
        "        for _, g in df.groupby(\"Subject\"):\n",
        "            g = g.sort_values(\"EventDateTime\")\n",
        "            arr = g[\"CGM_scaled\"].values.astype(np.float32)\n",
        "            trends = g[\"trend\"].astype(int).values\n",
        "            for i in range(len(arr) - seq_in):\n",
        "                x = arr[i:i+seq_in]\n",
        "                y = trends[i+seq_in]\n",
        "                self.samples.append((x, int(y)))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32).unsqueeze(-1), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Model\n",
        "class HybridGRUTransformerMTL(nn.Module):\n",
        "    def __init__(self, heart_feat_dim: int):\n",
        "        super().__init__()\n",
        "        # sequence path (for diabetes)\n",
        "        self.gru = nn.GRU(input_size=1, hidden_size=GRU_HIDDEN, num_layers=GRU_LAYERS,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.proj = nn.Linear(GRU_HIDDEN * 2, TRANS_DMODEL)\n",
        "        enc = nn.TransformerEncoderLayer(d_model=TRANS_DMODEL, nhead=TRANS_NHEAD,\n",
        "                                         dim_feedforward=TRANS_DMODEL*2, batch_first=True, dropout=DROPOUT)\n",
        "        self.transformer = nn.TransformerEncoder(enc, num_layers=TRANS_LAYERS)\n",
        "        self.seq_norm = nn.LayerNorm(TRANS_DMODEL)\n",
        "\n",
        "        # tabular path (for heart)\n",
        "        self.heart_mlp = nn.Sequential(\n",
        "            nn.Linear(heart_feat_dim, HEART_HIDDEN[0]),\n",
        "            nn.BatchNorm1d(HEART_HIDDEN[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT),\n",
        "            nn.Linear(HEART_HIDDEN[0], HEART_HIDDEN[1]),\n",
        "            nn.BatchNorm1d(HEART_HIDDEN[1]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # fusion + heads\n",
        "        fused = HEART_HIDDEN[1] + TRANS_DMODEL\n",
        "        self.combined_proj = nn.Sequential(\n",
        "            nn.Linear(fused, fused // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.heart_head = nn.Sequential(\n",
        "            nn.Linear(fused // 2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT),\n",
        "            nn.Linear(64, 5)   # 5 classes\n",
        "        )\n",
        "        self.diab_head = nn.Sequential(\n",
        "            nn.Linear(TRANS_DMODEL, DIAB_HEAD_HIDDEN),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT),\n",
        "            nn.Linear(DIAB_HEAD_HIDDEN, 3)  # 3 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, heart_x, diab_x):\n",
        "        # diabetes sequence → GRU → Transformer → last token\n",
        "        g, _ = self.gru(diab_x)       # [B, T, 2H]\n",
        "        p = self.proj(g)              # [B, T, D]\n",
        "        t = self.transformer(p)       # [B, T, D]\n",
        "        t = self.seq_norm(t)\n",
        "        diab_emb = t[:, -1, :]        # [B, D]\n",
        "        diab_logits = self.diab_head(diab_emb)\n",
        "\n",
        "        # heart tabular → MLP → fuse with diabetes context (broadcast mean if batch-size mismatch)\n",
        "        heart_feat = self.heart_mlp(heart_x)\n",
        "        if heart_x.size(0) == diab_x.size(0):\n",
        "            context = diab_emb\n",
        "        else:\n",
        "            context = diab_emb.mean(dim=0, keepdim=True).expand(heart_x.size(0), -1)\n",
        "        fused = torch.cat([heart_feat, context], dim=-1)\n",
        "        z = self.combined_proj(fused)\n",
        "        heart_logits = self.heart_head(z)\n",
        "        return heart_logits, diab_logits\n",
        "\n",
        "\n",
        "# Losses / KD\n",
        "\n",
        "def ce_with_label_smoothing(logits, targets, class_weights=None, smoothing=0.0):\n",
        "    # PyTorch CE supports label_smoothing natively\n",
        "    return F.cross_entropy(logits, targets, weight=class_weights, label_smoothing=smoothing)\n",
        "\n",
        "def kd_loss(student_logits, teacher_logits, T):\n",
        "    s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    return F.kl_div(s, t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "# Local training (per client)\n",
        "\n",
        "def train_local_one_client(heart_csv, diab_csv, out_prefix, epochs=LOCAL_EPOCHS) -> Dict:\n",
        "    print(f\"\\n=== LOCAL TRAINING @ {out_prefix} ===\")\n",
        "    # heart\n",
        "    heart_df = pd.read_csv(heart_csv).reset_index(drop=True)\n",
        "    heart_scaler = StandardScaler().fit(heart_df.drop(columns=[\"target\"]))\n",
        "    heart_ds = HeartDataset(heart_df, scaler=heart_scaler, fit_scaler=False)\n",
        "\n",
        "    # imbalance handling\n",
        "    y_np = heart_df[\"target\"].astype(int).values\n",
        "    cls_w = make_class_weights(y_np, num_classes=5).to(DEVICE)\n",
        "    # weighted sampler increases minority sampling\n",
        "    class_counts = np.bincount(y_np, minlength=5).astype(np.float32)\n",
        "    samp_weights = (1.0 / class_counts)[y_np]\n",
        "    sampler = WeightedRandomSampler(weights=samp_weights, num_samples=len(y_np), replacement=True)\n",
        "    heart_loader = DataLoader(heart_ds, batch_size=BATCH_HEART, sampler=sampler, drop_last=False)\n",
        "\n",
        "    # diabetes\n",
        "    diab_df_raw = pd.read_csv(diab_csv)\n",
        "    diab_df, sub_stats = compute_trend_labels(diab_df_raw)\n",
        "    diab_ds = DiabetesSeqDataset(diab_df, seq_in=DIAB_SEQ_IN)\n",
        "    diab_loader = DataLoader(diab_ds, batch_size=BATCH_DIAB, shuffle=True, drop_last=False)\n",
        "\n",
        "    heart_dim = heart_df.drop(columns=[\"target\"]).shape[1]\n",
        "    model = HybridGRUTransformerMTL(heart_feat_dim=heart_dim).to(DEVICE)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR_BASE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    model.train()\n",
        "    steps = max(len(heart_loader), len(diab_loader))\n",
        "    h_iter = iter(heart_loader); d_iter = iter(diab_loader)\n",
        "    for ep in range(1, epochs+1):\n",
        "        run_h = 0.0; run_d = 0.0\n",
        "        for _ in range(steps):\n",
        "            try:    hx, hy = next(h_iter)\n",
        "            except StopIteration: h_iter = iter(heart_loader); hx, hy = next(h_iter)\n",
        "            try:    dx, dy = next(d_iter)\n",
        "            except StopIteration: d_iter = iter(diab_loader); dx, dy = next(d_iter)\n",
        "            hx, hy, dx, dy = hx.to(DEVICE), hy.to(DEVICE), dx.to(DEVICE), dy.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            log_h, log_d = model(hx, dx)\n",
        "            lh = ce_with_label_smoothing(log_h, hy, class_weights=cls_w, smoothing=LABEL_SMOOTH)\n",
        "            ld = F.cross_entropy(log_d, dy)\n",
        "            loss = lh + ld\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "            opt.step()\n",
        "            run_h += lh.item(); run_d += ld.item()\n",
        "\n",
        "        print(f\"Client {out_prefix} ep {ep}/{epochs} | heart_loss {run_h/steps:.4f} | diab_loss {run_d/steps:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    # save artifacts\n",
        "    state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "    torch.save(state, os.path.join(ARTIFACT_DIR, f\"local_{out_prefix}_state.pt\"))\n",
        "    joblib.dump(heart_scaler, os.path.join(ARTIFACT_DIR, f\"scaler_heart_{out_prefix}.pkl\"))\n",
        "    joblib.dump(sub_stats,   os.path.join(ARTIFACT_DIR, f\"sub_stats_{out_prefix}.pkl\"))\n",
        "    return {\"state_dict\": state, \"heart_scaler\": heart_scaler, \"sub_stats\": sub_stats, \"heart_dim\": heart_dim}\n",
        "\n",
        "# CKD cycle\n",
        "\n",
        "def run_ckd_cycle(teacher_art, student_art, s_heart_csv, s_diab_csv, out_prefix, epochs=CKD_EPOCHS_PER_CYCLE) -> Dict:\n",
        "    print(f\"\\n--- CKD Teacher→Student ({out_prefix}) ---\")\n",
        "    heart_df = pd.read_csv(s_heart_csv)\n",
        "    heart_scaler = student_art[\"heart_scaler\"]\n",
        "    heart_ds = HeartDataset(heart_df, scaler=heart_scaler, fit_scaler=False)\n",
        "\n",
        "    # class weights (student's distribution)\n",
        "    y_np = heart_df[\"target\"].astype(int).values\n",
        "    cls_w = make_class_weights(y_np, num_classes=5).to(DEVICE)\n",
        "    # no need weighted sampler here; KD stabilizes; we still use simple shuffle loader\n",
        "    heart_loader = DataLoader(heart_ds, batch_size=BATCH_HEART, shuffle=True, drop_last=False)\n",
        "\n",
        "    diab_df_raw = pd.read_csv(s_diab_csv)\n",
        "    diab_df, _ = compute_trend_labels(diab_df_raw)\n",
        "    diab_ds = DiabetesSeqDataset(diab_df, seq_in=DIAB_SEQ_IN)\n",
        "    diab_loader = DataLoader(diab_ds, batch_size=BATCH_DIAB, shuffle=True, drop_last=False)\n",
        "\n",
        "    heart_dim = student_art[\"heart_dim\"]\n",
        "    teacher = HybridGRUTransformerMTL(heart_feat_dim=heart_dim).to(DEVICE)\n",
        "    student = HybridGRUTransformerMTL(heart_feat_dim=heart_dim).to(DEVICE)\n",
        "    teacher.load_state_dict(student_art[\"state_dict\"])\n",
        "    student.load_state_dict(student_art[\"state_dict\"])\n",
        "\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=LR_CKD, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    steps = max(len(heart_loader), len(diab_loader))\n",
        "    h_iter = iter(heart_loader); d_iter = iter(diab_loader)\n",
        "    for ep in range(1, epochs+1):\n",
        "        run = 0.0\n",
        "        for _ in range(steps):\n",
        "            try:    hx, hy = next(h_iter)\n",
        "            except StopIteration: h_iter = iter(heart_loader); hx, hy = next(h_iter)\n",
        "            try:    dx, dy = next(d_iter)\n",
        "            except StopIteration: d_iter = iter(diab_loader); dx, dy = next(d_iter)\n",
        "            hx, hy, dx, dy = hx.to(DEVICE), hy.to(DEVICE), dx.to(DEVICE), dy.to(DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                t_h, t_d = teacher(hx, dx)\n",
        "\n",
        "            s_h, s_d = student(hx, dx)\n",
        "            hard_h = ce_with_label_smoothing(s_h, hy, class_weights=cls_w, smoothing=LABEL_SMOOTH)\n",
        "            hard_d = F.cross_entropy(s_d, dy)\n",
        "            kd_h = kd_loss(s_h, t_h, KD_TEMPERATURE)\n",
        "            kd_d = kd_loss(s_d, t_d, KD_TEMPERATURE)\n",
        "            loss = (1-KD_WEIGHT)*(hard_h + hard_d) + KD_WEIGHT*(kd_h + kd_d)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(student.parameters(), CLIP_GRAD)\n",
        "            opt.step()\n",
        "            run += loss.item()\n",
        "        print(f\" CKD ep {ep}/{epochs} | loss {run/steps:.4f}\")\n",
        "\n",
        "    state = {k: v.cpu() for k, v in student.state_dict().items()}\n",
        "    return {\"state_dict\": state, \"heart_scaler\": heart_scaler, \"sub_stats\": student_art[\"sub_stats\"], \"heart_dim\": heart_dim}\n",
        "\n",
        "# Personalization with val split + early stopping (heart-first objective)\n",
        "\n",
        "def personalize_client(artifact, heart_csv, diab_csv, out_prefix, epochs=PERSONAL_EPOCHS, patience=PATIENCE):\n",
        "    print(f\"\\n=== PERSONALIZING {out_prefix} ===\")\n",
        "    heart_df = pd.read_csv(heart_csv).reset_index(drop=True)\n",
        "    scaler = artifact[\"heart_scaler\"]\n",
        "\n",
        "    # stratified split\n",
        "    y = heart_df[\"target\"].astype(int).values\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=SEED)\n",
        "    train_idx, val_idx = next(sss.split(heart_df, y))\n",
        "    heart_tr = heart_df.iloc[train_idx].reset_index(drop=True)\n",
        "    heart_va = heart_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    # datasets & loaders\n",
        "    ds_tr = HeartDataset(heart_tr, scaler=scaler, fit_scaler=False)\n",
        "    ds_va = HeartDataset(heart_va, scaler=scaler, fit_scaler=False)\n",
        "    # keep sampler in personalization too\n",
        "    y_tr = heart_tr[\"target\"].astype(int).values\n",
        "    cls_w = make_class_weights(y_tr, 5).to(DEVICE)\n",
        "    class_counts = np.bincount(y_tr, minlength=5).astype(np.float32); class_counts[class_counts==0]=1\n",
        "    samp_w = (1.0 / class_counts)[y_tr]\n",
        "    sampler = WeightedRandomSampler(weights=samp_w, num_samples=len(y_tr), replacement=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_HEART, sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=256, shuffle=False)\n",
        "\n",
        "    # diabetes for joint stability\n",
        "    diab_df_raw = pd.read_csv(diab_csv)\n",
        "    diab_df, _ = compute_trend_labels(diab_df_raw)\n",
        "    diab_ds = DiabetesSeqDataset(diab_df, seq_in=DIAB_SEQ_IN)\n",
        "    dl_diab = DataLoader(diab_ds, batch_size=BATCH_DIAB, shuffle=True)\n",
        "\n",
        "    heart_dim = artifact[\"heart_dim\"]\n",
        "    model = HybridGRUTransformerMTL(heart_feat_dim=heart_dim).to(DEVICE)\n",
        "    model.load_state_dict(artifact[\"state_dict\"])\n",
        "\n",
        "    # freeze → unfreeze selected\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    for p in model.proj.parameters(): p.requires_grad = True\n",
        "    for p in model.transformer.parameters(): p.requires_grad = True\n",
        "    for p in model.heart_mlp.parameters(): p.requires_grad = True\n",
        "    for p in model.combined_proj.parameters(): p.requires_grad = True\n",
        "    for p in model.heart_head.parameters(): p.requires_grad = True\n",
        "    for p in model.diab_head.parameters(): p.requires_grad = True\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    opt = torch.optim.Adam(params, lr=LR_PERS, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    no_improve = 0\n",
        "\n",
        "    steps = max(len(dl_tr), len(dl_diab))\n",
        "    h_iter = iter(dl_tr); d_iter = iter(dl_diab)\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        run_h = 0.0; run_d = 0.0\n",
        "        for _ in range(steps):\n",
        "            try:    hx, hy = next(h_iter)\n",
        "            except StopIteration: h_iter = iter(dl_tr); hx, hy = next(h_iter)\n",
        "            try:    dx, dy = next(d_iter)\n",
        "            except StopIteration: d_iter = iter(dl_diab); dx, dy = next(d_iter)\n",
        "            hx, hy, dx, dy = hx.to(DEVICE), hy.to(DEVICE), dx.to(DEVICE), dy.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            log_h, log_d = model(hx, dx)\n",
        "            lh = ce_with_label_smoothing(log_h, hy, class_weights=cls_w, smoothing=LABEL_SMOOTH)\n",
        "            ld = F.cross_entropy(log_d, dy)\n",
        "            loss = lh + 0.5*ld\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(params, CLIP_GRAD)\n",
        "            opt.step()\n",
        "            run_h += lh.item(); run_d += ld.item()\n",
        "\n",
        "        # validate (heart only)\n",
        "        model.eval()\n",
        "        preds, gold = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(DEVICE)\n",
        "                logits, _ = model(xb, torch.zeros((xb.size(0), DIAB_SEQ_IN, 1), device=DEVICE))\n",
        "                preds.append(logits.argmax(1).cpu().numpy()); gold.append(yb.numpy())\n",
        "        preds = np.concatenate(preds); gold = np.concatenate(gold)\n",
        "        f1 = f1_score(gold, preds, average=\"macro\")\n",
        "        print(f\" Personalize ep {ep}/{epochs} | heart_loss {run_h/steps:.4f} | diab_loss {run_d/steps:.4f} | val_f1 {f1:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\" Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # load best & save\n",
        "    model.load_state_dict(best_state)\n",
        "    out_path = os.path.join(ARTIFACT_DIR, f\"personalized_{out_prefix}_state.pt\")\n",
        "    torch.save({k: v.cpu() for k, v in model.state_dict().items()}, out_path)\n",
        "    # scalers & stats already saved by local step, but we keep separate aliases too:\n",
        "    joblib.dump(artifact[\"heart_scaler\"], os.path.join(ARTIFACT_DIR, f\"personal_scaler_heart_{out_prefix}.pkl\"))\n",
        "    joblib.dump(artifact[\"sub_stats\"],   os.path.join(ARTIFACT_DIR, f\"personal_substats_{out_prefix}.pkl\"))\n",
        "    print(f\"Saved: {out_path}\")\n",
        "\n",
        "    return {\"state_dict\": {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "            \"heart_scaler\": artifact[\"heart_scaler\"],\n",
        "            \"sub_stats\": artifact[\"sub_stats\"],\n",
        "            \"heart_dim\": heart_dim}\n",
        "\n",
        "def run_training_pipeline():\n",
        "    t0 = time.time()\n",
        "    # Local training\n",
        "    artA = train_local_one_client(A_HEART_CSV, A_DIAB_CSV, out_prefix=\"A\", epochs=LOCAL_EPOCHS)\n",
        "    artB = train_local_one_client(B_HEART_CSV, B_DIAB_CSV, out_prefix=\"B\", epochs=LOCAL_EPOCHS)\n",
        "\n",
        "    # CKD cycles (A↔B)\n",
        "    studentA, studentB = artA, artB\n",
        "    for cyc in range(1, CKD_CYCLES+1):\n",
        "        print(f\"\\n========== CKD cycle {cyc}/{CKD_CYCLES} ==========\")\n",
        "        studentB = run_ckd_cycle(teacher_art=studentA, student_art=studentB,\n",
        "                                 s_heart_csv=B_HEART_CSV, s_diab_csv=B_DIAB_CSV,\n",
        "                                 out_prefix=\"B\", epochs=CKD_EPOCHS_PER_CYCLE)\n",
        "        torch.save(studentB[\"state_dict\"], os.path.join(ARTIFACT_DIR, f\"ckd_cycle{cyc}_B_state.pt\"))\n",
        "        studentA = run_ckd_cycle(teacher_art=studentB, student_art=studentA,\n",
        "                                 s_heart_csv=A_HEART_CSV, s_diab_csv=A_DIAB_CSV,\n",
        "                                 out_prefix=\"A\", epochs=CKD_EPOCHS_PER_CYCLE)\n",
        "        torch.save(studentA[\"state_dict\"], os.path.join(ARTIFACT_DIR, f\"ckd_cycle{cyc}_A_state.pt\"))\n",
        "\n",
        "    # Personalize (with early stopping)\n",
        "    persA = personalize_client(studentA, A_HEART_CSV, A_DIAB_CSV, out_prefix=\"A\", epochs=PERSONAL_EPOCHS, patience=PATIENCE)\n",
        "    persB = personalize_client(studentB, B_HEART_CSV, B_DIAB_CSV, out_prefix=\"B\", epochs=PERSONAL_EPOCHS, patience=PATIENCE)\n",
        "\n",
        "    # final aliases (for easy loading later)\n",
        "    torch.save(persA[\"state_dict\"], os.path.join(ARTIFACT_DIR, \"final_A_state.pt\"))\n",
        "    torch.save(persB[\"state_dict\"], os.path.join(ARTIFACT_DIR, \"final_B_state.pt\"))\n",
        "    joblib.dump(persA[\"heart_scaler\"], os.path.join(ARTIFACT_DIR, \"final_heart_scaler_A.pkl\"))\n",
        "    joblib.dump(persB[\"heart_scaler\"], os.path.join(ARTIFACT_DIR, \"final_heart_scaler_B.pkl\"))\n",
        "    joblib.dump(persA[\"sub_stats\"], os.path.join(ARTIFACT_DIR, \"final_sub_stats_A.pkl\"))\n",
        "    joblib.dump(persB[\"sub_stats\"], os.path.join(ARTIFACT_DIR, \"final_sub_stats_B.pkl\"))\n",
        "\n",
        "    print(f\"\\nAll stages complete. Artifacts → {ARTIFACT_DIR}\")\n",
        "    print(\"Elapsed (s):\", time.time() - t0)\n",
        "\n",
        "\n",
        "# RUN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdNQuLD94cXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6544bea7-92ca-4b3c-86d0-f0180bcd8136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ckd_cycle2_A_state.pt', 'ckd_cycle3_A_state.pt', 'final_heart_scaler_A.pkl', 'ckd_cycle3_B_state.pt', 'local_B_state.pt', 'personalized_B_state.pt', 'sub_stats_A.pkl', 'personal_scaler_heart_B.pkl', 'ckd_cycle1_B_state.pt', 'scaler_heart_A.pkl', 'final_sub_stats_B.pkl', 'personal_substats_B.pkl', 'personalized_A_state.pt', 'personal_scaler_heart_A.pkl', 'final_heart_scaler_B.pkl', 'local_A_state.pt', 'final_B_state.pt', 'personal_substats_A.pkl', 'sub_stats_B.pkl', 'final_sub_stats_A.pkl', 'scaler_heart_B.pkl', 'final_A_state.pt', 'ckd_cycle2_B_state.pt', 'ckd_cycle1_A_state.pt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"metafed_ckd_artifacts_final\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCv9-toqpT0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2121f6ff-8698-4e1d-a4d6-96bdc10af830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[+] Evaluating Client A\n",
            "\n",
            "===== HEART TEST REPORT =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.32      0.27        19\n",
            "           1       0.38      0.53      0.44        19\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       0.36      0.26      0.30        19\n",
            "           4       0.45      0.47      0.46        19\n",
            "\n",
            "    accuracy                           0.32        95\n",
            "   macro avg       0.28      0.32      0.30        95\n",
            "weighted avg       0.28      0.32      0.30        95\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209513802.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  heart_test = heart_test.apply(pd.to_numeric, errors=\"ignore\")\n",
            "/tmp/ipython-input-209513802.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  self.xs = torch.tensor(self.xs).float().unsqueeze(-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== DIABETES TEST REPORT =====\n",
            "Accuracy: 1.0\n",
            "Macro F1: 1.0\n",
            "\n",
            "[+] Evaluating Client B\n",
            "\n",
            "===== HEART TEST REPORT =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.42      0.34        19\n",
            "           1       0.07      0.11      0.08        19\n",
            "           2       0.00      0.00      0.00        19\n",
            "           3       0.27      0.32      0.29        19\n",
            "           4       0.62      0.26      0.37        19\n",
            "\n",
            "    accuracy                           0.22        95\n",
            "   macro avg       0.25      0.22      0.22        95\n",
            "weighted avg       0.25      0.22      0.22        95\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209513802.py:79: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  heart_test = heart_test.apply(pd.to_numeric, errors=\"ignore\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== DIABETES TEST REPORT =====\n",
            "Accuracy: 1.0\n",
            "Macro F1: 1.0\n"
          ]
        }
      ],
      "source": [
        "                    #TESTING METAFED.......\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import joblib\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----- PATHS -----\n",
        "ART = \"metafed_ckd_artifacts_final\"\n",
        "HEART_TEST = \"heart_test_balanced.csv\"\n",
        "DIAB_TEST = \"diabetes_test.csv\"   # old diabetes test remains same\n",
        "\n",
        "# ----- DATASET CLASSES -----\n",
        "class HeartDataset(Dataset):\n",
        "    def __init__(self, df, scaler):\n",
        "        X = scaler.transform(df.drop(columns=[\"target\"]))\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(df[\"target\"].astype(int).values, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class DiabetesSeqTest(Dataset):\n",
        "    def __init__(self, df, seq_len=30):\n",
        "        self.xs = []\n",
        "        self.ys = []\n",
        "        df = df.sort_values(\"EventDateTime\")\n",
        "        arr = df[\"CGM_scaled\"].values\n",
        "        lab = df[\"trend\"].values\n",
        "        for i in range(len(arr)-seq_len):\n",
        "            self.xs.append(arr[i:i+seq_len])\n",
        "            self.ys.append(lab[i+seq_len])\n",
        "        self.xs = torch.tensor(self.xs).float().unsqueeze(-1)\n",
        "        self.ys = torch.tensor(self.ys).long()\n",
        "\n",
        "    def __len__(self): return len(self.ys)\n",
        "    def __getitem__(self, idx): return self.xs[idx], self.ys[idx]\n",
        "\n",
        "# ----- PREP DIABETES DATA -----\n",
        "def prepare_diabetes(df):\n",
        "    df = df.copy()\n",
        "    df[\"CGM\"] = pd.to_numeric(df[\"CGM\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"CGM\"]).reset_index(drop=True)\n",
        "\n",
        "    df[\"Subject\"] = df[\"Subject\"].astype(str)\n",
        "    df = df.sort_values([\"Subject\", \"EventDateTime\"]).reset_index(drop=True)\n",
        "\n",
        "    df[\"CGM_scaled\"] = df.groupby(\"Subject\")[\"CGM\"].transform(lambda x: (x - x.mean())/(x.std()+1e-6))\n",
        "\n",
        "    slopes = []\n",
        "    w = 12\n",
        "    for subj, g in df.groupby(\"Subject\"):\n",
        "        s = g[\"CGM_scaled\"].values\n",
        "        tvals = []\n",
        "        for i in range(len(s)):\n",
        "            if i < w: tvals.append(0)\n",
        "            else:\n",
        "                sl = (s[i] - s[i-w]) / w\n",
        "                tvals.append(1 if sl>0.5 else 2 if sl<-0.5 else 0)\n",
        "        slopes.extend(tvals)\n",
        "\n",
        "    df[\"trend\"] = slopes\n",
        "    return df\n",
        "\n",
        "# ----- MAIN EVALUATION -----\n",
        "def evaluate(prefix):\n",
        "    print(f\"\\n[+] Evaluating Client {prefix}\")\n",
        "\n",
        "    # Load heart scaler\n",
        "    scaler = joblib.load(f\"{ART}/scaler_heart_{prefix}.pkl\")\n",
        "\n",
        "    # Load heart test + clean\n",
        "    heart_test = pd.read_csv(HEART_TEST)\n",
        "    heart_test = heart_test.replace(\"?\", np.nan)\n",
        "    heart_test = heart_test.apply(pd.to_numeric, errors=\"ignore\")\n",
        "    for col in heart_test.columns:\n",
        "        if col != \"target\" and heart_test[col].dtype != object:\n",
        "            heart_test[col] = heart_test[col].fillna(heart_test[col].mean())\n",
        "\n",
        "    heart_ds = HeartDataset(heart_test, scaler)\n",
        "    heart_loader = DataLoader(heart_ds, batch_size=64)\n",
        "\n",
        "    # Load weights\n",
        "    state = torch.load(f\"{ART}/personalized_{prefix}_state.pt\", map_location=DEVICE)\n",
        "\n",
        "    # Build model\n",
        "    feat_dim = heart_test.drop(columns=[\"target\"]).shape[1]\n",
        "    model = HybridGRUTransformerMTL(feat_dim).to(DEVICE)\n",
        "\n",
        "    # ✅ FIX: allow mismatched keys\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # ----- HEART PRED -----\n",
        "    yh_true, yh_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in heart_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            logits_h, _ = model(x, torch.zeros((x.size(0), 30, 1), device=DEVICE))\n",
        "            yh_true.extend(y.numpy())\n",
        "            yh_pred.extend(logits_h.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    print(\"\\n===== HEART TEST REPORT =====\")\n",
        "    print(classification_report(yh_true, yh_pred))\n",
        "\n",
        "\n",
        "    # ----- DIABETES TEST -----\n",
        "    diab_test = prepare_diabetes(pd.read_csv(DIAB_TEST))\n",
        "    diab_ds = DiabetesSeqTest(diab_test)\n",
        "    diab_loader = DataLoader(diab_ds, batch_size=256)\n",
        "\n",
        "    dy_true, dy_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in diab_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            _, logits_d = model(torch.zeros((x.size(0), feat_dim), device=DEVICE), x)\n",
        "            dy_true.extend(y.numpy())\n",
        "            dy_pred.extend(logits_d.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    print(\"\\n===== DIABETES TEST REPORT =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(dy_true, dy_pred))\n",
        "    print(\"Macro F1:\", f1_score(dy_true, dy_pred, average=\"macro\"))\n",
        "\n",
        "# Run both\n",
        "evaluate(\"A\")\n",
        "evaluate(\"B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWmW7R6p-STj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e47a52-0af4-43cb-fe0e-663f232d1681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- VALIDATION: CLIENT A -----\n",
            "Heart Accuracy: 0.5273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-60135791.py:70: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  hv = hv.apply(pd.to_numeric, errors=\"ignore\").fillna(hv.mean())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes Accuracy: 1.0000\n",
            "\n",
            "----- VALIDATION: CLIENT B -----\n",
            "Heart Accuracy: 0.2909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-60135791.py:70: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  hv = hv.apply(pd.to_numeric, errors=\"ignore\").fillna(hv.mean())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ART = \"metafed_ckd_artifacts_final\"\n",
        "HEART_VAL = \"heart_val_balanced.csv\"\n",
        "DIAB_VAL = \"diabetes_val.csv\"\n",
        "\n",
        "# ---------------- HEART DATASET ----------------\n",
        "class HeartDataset(Dataset):\n",
        "    def __init__(self, df, scaler):\n",
        "        X = scaler.transform(df.drop(columns=[\"target\"]))\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(df[\"target\"].astype(int).values, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "# ---------------- DIABETES SEQ DATASET ----------------\n",
        "class DiabetesSeqVal(Dataset):\n",
        "    def __init__(self, df, seq=30):\n",
        "        xs, ys = [], []\n",
        "        for _, g in df.groupby(\"Subject\"):\n",
        "            g = g.sort_values(\"EventDateTime\")\n",
        "            arr = g[\"CGM_scaled\"].values\n",
        "            lab = g[\"trend\"].values\n",
        "            for i in range(len(arr)-seq):\n",
        "                xs.append(arr[i:i+seq])\n",
        "                ys.append(lab[i+seq])\n",
        "        self.X = torch.tensor(xs).float().unsqueeze(-1)\n",
        "        self.Y = torch.tensor(ys).long()\n",
        "    def __len__(self): return len(self.Y)\n",
        "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
        "\n",
        "# ---------------- DIABETES PREPROCESS ----------------\n",
        "def prepare_diabetes(df):\n",
        "    df = df.copy()\n",
        "    df[\"CGM\"] = pd.to_numeric(df[\"CGM\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"CGM\"]).reset_index(drop=True)\n",
        "    df[\"Subject\"] = df[\"Subject\"].astype(str)\n",
        "    df = df.sort_values([\"Subject\", \"EventDateTime\"]).reset_index(drop=True)\n",
        "    df[\"CGM_scaled\"] = df.groupby(\"Subject\")[\"CGM\"].transform(lambda x: (x-x.mean())/(x.std()+1e-6))\n",
        "\n",
        "    slopes = []\n",
        "    w = 12\n",
        "    for subj, g in df.groupby(\"Subject\"):\n",
        "        s = g[\"CGM_scaled\"].values\n",
        "        t = []\n",
        "        for i in range(len(s)):\n",
        "            if i < w: t.append(0)\n",
        "            else:\n",
        "                sl = (s[i]-s[i-w])/w\n",
        "                t.append(1 if sl>0.5 else 2 if sl<-0.5 else 0)\n",
        "        slopes.extend(t)\n",
        "    df[\"trend\"] = slopes\n",
        "    return df\n",
        "\n",
        "# ---------------- VALIDATION FUNCTION ----------------\n",
        "def validate(prefix):\n",
        "    print(f\"\\n----- VALIDATION: CLIENT {prefix} -----\")\n",
        "\n",
        "    scaler = joblib.load(f\"{ART}/scaler_heart_{prefix}.pkl\")\n",
        "    state = torch.load(f\"{ART}/personalized_{prefix}_state.pt\", map_location=DEVICE)\n",
        "\n",
        "    hv = pd.read_csv(HEART_VAL).replace(\"?\", np.nan)\n",
        "    hv = hv.apply(pd.to_numeric, errors=\"ignore\").fillna(hv.mean())\n",
        "    heart_ds = HeartDataset(hv, scaler)\n",
        "    heart_loader = DataLoader(heart_ds, batch_size=64)\n",
        "\n",
        "    feat_dim = hv.drop(columns=[\"target\"]).shape[1]\n",
        "    model = HybridGRUTransformerMTL(feat_dim).to(DEVICE)\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # Heart\n",
        "    yh_true, yh_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in heart_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            logits, _ = model(x, torch.zeros((x.size(0), 30, 1), device=DEVICE))\n",
        "            yh_true.extend(y.numpy())\n",
        "            yh_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "\n",
        "    print(f\"Heart Accuracy: {accuracy_score(yh_true, yh_pred):.4f}\")\n",
        "\n",
        "    # Diabetes\n",
        "    dv = prepare_diabetes(pd.read_csv(DIAB_VAL))\n",
        "    diab_ds = DiabetesSeqVal(dv)\n",
        "    diab_loader = DataLoader(diab_ds, batch_size=256)\n",
        "\n",
        "    dy_true, dy_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in diab_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            _, logits = model(torch.zeros((x.size(0), feat_dim), device=DEVICE), x)\n",
        "            dy_true.extend(y.numpy())\n",
        "            dy_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "\n",
        "    print(f\"Diabetes Accuracy: {accuracy_score(dy_true, dy_pred):.4f}\")\n",
        "\n",
        "# ---------------- RUN ----------------\n",
        "validate(\"A\")\n",
        "validate(\"B\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}